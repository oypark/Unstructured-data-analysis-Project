{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98206641-3e34-48ff-8331-01597f20676b",
   "metadata": {},
   "source": [
    "# CNN for Sentence Classification 논문 구현\n",
    "\n",
    "#### Git - Commit Message Convention\n",
    "\n",
    "* 제가 대략적으로 framework을 잡아놓은 것이니, 담당하시는 부분에 수정이 필요하시면 마크다운 양식만 유지한 채 수정해주시면 됩니다!\n",
    "* 작은 단위의 작업이 끝날 때 마다 git add, commit, push 해주시면 됩니다! (push하고 슬랙에 공유 부탁드려요! 화이팅!)\n",
    "\n",
    "* git commit message는 다음의 양식을 참고해주세요!\n",
    "    * 처음으로 코드 완료했을 때 git commit -m \"동사 명사\"\n",
    "    ```ex) git commit -m \"Fill and replace NaN values\"```\n",
    "    \n",
    "    * commit 했던 코드를 수정했을 때 git commit -m \"Update 수정한 내용\"\n",
    "    ```ex) git commit -m \"Update Word embedding\"```\n",
    "\n",
    "\n",
    "# 1. 데이터 load 및 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cedc75-0209-41ef-8a49-c72082488438",
   "metadata": {},
   "source": [
    "## 1) 네이버 영화 리뷰 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11fa2558-9cec-4cd7-a44e-6c23587cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7834b971-bdd0-4b68-8c45-e8c1ff5f72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"ratings_test.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e747db2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33668ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()    #document에 null값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0643ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        50000 non-null  int64 \n",
      " 1   document  49997 non-null  object\n",
      " 2   label     50000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()    #document에 null값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c875bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>2172111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55737</th>\n",
       "      <td>6369843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110014</th>\n",
       "      <td>1034280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126782</th>\n",
       "      <td>5942978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140721</th>\n",
       "      <td>1034283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id document  label\n",
       "25857   2172111      NaN      1\n",
       "55737   6369843      NaN      1\n",
       "110014  1034280      NaN      0\n",
       "126782  5942978      NaN      0\n",
       "140721  1034283      NaN      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['document'].isnull()]     #갯수가 별로 없고 대체할 방법이 없으므로 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc573d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결측치 제거\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcd4cd-1fc7-4f2f-a609-5b538302d128",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Sentences, labels 생성\n",
    "\n",
    "* sentence는 tokenize하고 나중에 생성 ☆\n",
    "* label만 numpy 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53279a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_labels = np.array(train.label)\n",
    "test_labels = np.array(test.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756f22a-c9b0-496a-888f-06dfbd6bea49",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6281bf7-fb7a-4b01-9ca9-79e4c6220e5f",
   "metadata": {},
   "source": [
    "## 1) Tokenizer\n",
    "\n",
    "* 어절 단위\n",
    "* 형태소 단위 - Mecab 활용\n",
    "* Subword 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47e24b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어, 한글만 포함하고 나머지 제거\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "  text = re.sub(r\"[^A-Za-zㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", text) \n",
    "  return text\n",
    "\n",
    "train['document'] = train.document.apply(lambda x : preprocess(x))\n",
    "test['document'] = test.document.apply(lambda x : preprocess(x))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d17c929-0889-4f20-82ec-9c521df9cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab으로 형태소 분석, 불용어 제거\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']   #korean_stopwords 보류\n",
    "\n",
    "train_tokens = []\n",
    "test_tokens = []\n",
    "\n",
    "for sentence in train['document']:\n",
    "    tokenized_sentence = tokenizer.morphs(sentence) \n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
    "    if len(stopwords_removed_sentence) != 0:\n",
    "        train_tokens.append(stopwords_removed_sentence)\n",
    "\n",
    "for sentence in test['document']:\n",
    "    tokenized_sentence = tokenizer.morphs(sentence) \n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
    "    if len(stopwords_removed_sentence) != 0:\n",
    "        test_tokens.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2cc1e9f-42e1-49d9-8bb9-1c239ea2a9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['아', '더', '빙', '진짜', '짜증', '나', '네요', '목소리'],\n",
       " ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍', '지', '않', '구나'],\n",
       " ['너무', '재', '밓었다그래서보는것을추천한다'],\n",
       " ['교도소', '이야기', '구먼', '솔직히', '재미', '없', '다', '평점', '조정'],\n",
       " ['사이몬페그',\n",
       "  '익살',\n",
       "  '스런',\n",
       "  '연기',\n",
       "  '돋보였',\n",
       "  '던',\n",
       "  '영화',\n",
       "  '스파이더맨',\n",
       "  '에서',\n",
       "  '늙',\n",
       "  '어',\n",
       "  '보이',\n",
       "  '기',\n",
       "  '만',\n",
       "  '했',\n",
       "  '던',\n",
       "  '커스틴',\n",
       "  '던스트',\n",
       "  '너무나',\n",
       "  '이뻐',\n",
       "  '보였',\n",
       "  '다'],\n",
       " ['막',\n",
       "  '걸음마',\n",
       "  '뗀',\n",
       "  '세',\n",
       "  '부터',\n",
       "  '초등',\n",
       "  '학교',\n",
       "  '학년',\n",
       "  '생',\n",
       "  '인',\n",
       "  '살용',\n",
       "  '영화',\n",
       "  'ㅋㅋㅋ',\n",
       "  '별반',\n",
       "  '개',\n",
       "  '아까움'],\n",
       " ['원작', '긴장감', '을', '제대로', '살려', '내', '지', '못했', '다'],\n",
       " ['별',\n",
       "  '반개',\n",
       "  '아깝',\n",
       "  '다',\n",
       "  '욕',\n",
       "  '나온다',\n",
       "  '이응경',\n",
       "  '길용우',\n",
       "  '연기',\n",
       "  '생활',\n",
       "  '몇',\n",
       "  '년',\n",
       "  '인지',\n",
       "  '정말',\n",
       "  '발',\n",
       "  '로',\n",
       "  '해도',\n",
       "  '그것',\n",
       "  '보단',\n",
       "  '낫',\n",
       "  '겟',\n",
       "  '다',\n",
       "  '납치',\n",
       "  '감금',\n",
       "  '만',\n",
       "  '반복',\n",
       "  '반복',\n",
       "  '드라마',\n",
       "  '가족',\n",
       "  '없',\n",
       "  '다',\n",
       "  '연기',\n",
       "  '못',\n",
       "  '하',\n",
       "  '사람',\n",
       "  '만',\n",
       "  '모엿',\n",
       "  '네'],\n",
       " ['액션', '없', '는데', '재미', '있', '몇', '안', '되', '영화'],\n",
       " ['왜',\n",
       "  '케',\n",
       "  '평점',\n",
       "  '낮',\n",
       "  '건데',\n",
       "  '꽤',\n",
       "  '볼',\n",
       "  '만',\n",
       "  '한데',\n",
       "  '헐리우드',\n",
       "  '식',\n",
       "  '화려',\n",
       "  '함',\n",
       "  '만',\n",
       "  '너무',\n",
       "  '길들여져',\n",
       "  '있',\n",
       "  '나']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bebc6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['굳', 'ㅋ'],\n",
       " ['GDNTOPCLASSINTHECLUB'],\n",
       " ['뭐', '야', '평점', '나쁘', '진', '않', '지만', '점', '짜리', '더더욱', '아니', '잖아']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba4c35-9e1d-441f-b694-01523dd8d2ad",
   "metadata": {},
   "source": [
    "## 2) 리뷰 데이터 EDA\n",
    "\n",
    "* 문장 histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c90e4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_len = [len(tokens) for tokens in train_tokens]\n",
    "test_token_len = [len(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2539bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_token_len, bins=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f942393d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_token_len), min(train_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50d537d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.55445928421503"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_token_len)/len(train_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "586ebfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## max_length 길이 정하기\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780eb52-5f9c-40a6-8a34-6cb685d3c428",
   "metadata": {},
   "source": [
    "## 3) Word Vectorize\n",
    "\n",
    "* word embedding : 문장들을 word vector 형태로 변환\n",
    "    * 윗단에서 tokenizer output을 '문장' 형태라고 가정하고 코드 작업\n",
    "    1. 문장을 토큰으로 쪼갠다\n",
    "    2. 쪼개진 토큰을 가장 긴 문장에 맞춰 패딩한다\n",
    "    3. 패딩이 마친 토큰들을 word vector로 변환하다\n",
    "* oov, padding, truncating 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320cc1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.3\n",
      "  Using cached tensorflow-2.3.0-cp37-cp37m-win_amd64.whl (342.5 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (0.37.1)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (1.1.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (1.13.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (2.3.0)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (3.19.4)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.44.0-cp37-cp37m-win_amd64.whl (3.4 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow==2.3) (1.18.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.6.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (58.0.4)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.8.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.10.0.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dhfps\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.2.0)\n",
      "Installing collected packages: scipy, opt-einsum, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 gast-0.3.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.6 opt-einsum-3.3.0 requests-oauthlib-1.3.1 scipy-1.4.1 tensorboard-2.8.0 tensorflow-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.18.5 which is incompatible.\n",
      "WARNING: You are using pip version 22.0.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\dhfps\\anaconda3\\envs\\nlp\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install tensorflow==2.3\n",
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36ab7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [\" \".join(tokens) for tokens in train_tokens]\n",
    "test_sentences = [\" \".join(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c48ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "deeb1ca2-81d0-49e7-81f5-89cc899a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocabulary, sequences 생성 및 padding 함수\n",
    "\n",
    "def word_vectorize(sentences, max_length, oov_tok='<oov>', pad_type='post', trunc_type='post'):\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token=\"<oov>\")    #Tokenizer 객체 생성\n",
    "    tokenizer.fit_on_texts(train_sentences)    #train sentences 입력\n",
    "    word_index = tokenizer.word_index           #word : index 사전(vocab) 생성\n",
    "    sequences = tokenizer.texts_to_sequences(train_sentences)     #시퀀스 생성\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=trunc_type, truncating=trunc_type)   #padding, truncating\n",
    "\n",
    "    return word_index, sequences, padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "141b5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index, sequences, padded = word_vectorize(train_sentences, max_length)\n",
    "word_index_test, sequences_test, padded_test = word_vectorize(test_sentences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1df9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {value : key for (key, value) in word_index.items()}    #index : word 사전\n",
    "\n",
    "## 시퀀스 decoding 함수\n",
    "def decode_review(text):\n",
    "  return \" \".join([index_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08ea15d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 65, 912, 31, 214, 13, 27, 705]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]    # 첫 번째 시퀀스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3438daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더 빙 진짜 짜증 나 네요 목소리 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(padded[0])    # padded 된 시퀀스 -> decode해서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c19b7e-00b5-43d7-9c02-bbc1de8d17a1",
   "metadata": {},
   "source": [
    "## 3) Train, Valid, Test set 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f646841-1049-4d47-be6a-e705adfd54e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee662e32-fe98-4e9b-bf20-ecd6d282d347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f58a53ec-84c4-457c-85e2-9a8e90eaf583",
   "metadata": {},
   "source": [
    "# 3. Convolutional Neural Networks Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c313698-11ec-4c04-a93b-76c553fa2fe5",
   "metadata": {},
   "source": [
    "## 1) 기본 Model 생성\n",
    "\n",
    "* Check points\n",
    "* padding, initializer\n",
    "* activation function\n",
    "* Dropout\n",
    "* Batch Normalization\n",
    "* optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f060a89-c496-4543-a83c-2beb31467887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce441a6b-5006-4213-97ba-75a78980ca3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16814d-19f1-49ce-b49a-d64cddc17a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9057011-7b65-4947-8b98-dba32a7ad3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773d0dd-d874-4651-8ed8-976ddf4b26e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca4dcd76-0455-41ba-bb87-80bdf6967205",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Model 저장, Callback 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057d316-b82e-4c46-9232-ff9af8efa482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949f7d1-9e51-468c-b321-9ab33303c329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a7220-3c7f-4c7b-9cf2-40c265e57501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d76a76e5",
   "metadata": {},
   "source": [
    "### FastText pre-trained model 불러오기\n",
    "\n",
    "* 1차시도 : https://github.com/Kyubyong/wordvectors - 성공!\n",
    "    * korean fasttext file download 후 ko.bin(word vector + model) 활용\n",
    "* 2차시도 : https://fasttext.cc/docs/en/crawl-vectors.html - facebook에서 제공하는 기본 pre-trained 모델 (성능이 별로 좋지 않다고 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570aa358-e4ec-4292-89d2-922247f7bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load fasttext START at 2022-03-10 09:41:03.933106\n",
      "Load fasttext   END at 2022-03-10 09:41:18.819998\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from gensim.models import fasttext\n",
    "\n",
    "print(f\"Load fasttext START at {datetime.datetime.now()}\")\n",
    "model = fasttext.load_facebook_model(\"ko.bin\")\n",
    "print(f\"Load fasttext   END at {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2f6a13-31c8-4637-9e95-18d89ab8e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('선생', 0.5959748029708862), ('친구', 0.5727326273918152), ('엄마', 0.5673928260803223), ('교실', 0.5516300797462463), ('학년', 0.5418106913566589), ('학교의', 0.5392650961875916), ('깡', 0.5327043533325195), ('교장', 0.5243589878082275), ('여학생', 0.5234997272491455), ('교사', 0.5217373967170715)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('선생님'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e7d7a-3526-4a0b-8e34-ea64d18e951c",
   "metadata": {},
   "source": [
    "# 4. Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1b73d-c341-4973-a5b6-5fc8ab51030b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62be35-7a2f-4c40-a225-72f00a6bc7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab5b19-ac27-4851-ac9b-d3051aae695a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07364a50-7fa0-4f85-97c5-4f669491166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
