# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13lvRK-uz5Vs9r_Qf5bqygxWHkmpnIJTU
"""

import pandas as pd
import numpy as np

train_data = pd.read_csv('/content/drive/MyDrive/실습/ratings_train.txt', sep='\t')
test_data = pd.read_csv('/content/drive/MyDrive/실습/ratings_test.txt', sep='\t')
test_data.head()

test_data.isnull().sum()

# 결측치 제거 
train_data = train_data.dropna()
test_data = test_data.dropna()

train_data.isnull().sum()

print(len(train_data))
print(len(test_data))

# sentence와 label분리
train_sentences = train_data.document
test_sentences = test_data.document

train_labels = train_data.label
test_labels = test_data.label

# tensor 이용을 위해 numpy로
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

# max_length
max([len(s.split()) for s in train_data['document']])

import tensorflow as tf
from tensorflow.keras import layers, Sequential, Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_sentences)

vocab_size = len(tokenizer.word_index)
max_length = 41

# train data
sequences = tokenizer.texts_to_sequences(train_sentences)
padded = pad_sequences(sequences, maxlen=max_length)
# test data
test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=max_length)

padded[0]

