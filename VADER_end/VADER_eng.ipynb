{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f1ce40-6ad4-4fe8-b9dd-c05c2be110b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import codecs\n",
    "import json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7b395b-fe0e-4837-a3fc-b42d6784212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boost word가 감정 강도를 바꾸는 정도 (이론적으로 도출된 결과 값이라고 설명되어 있음) \n",
    "#boost word : ex) 매우, 극도로, 아주, 약간 등등 수식하는 단어의 강도를 나타내는 부사적 어휘\n",
    "#이 수치를 그대로 사용할 것인가...?\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "#영어의 경우 대문자로 강조 할 수 있음, 이떄 감정 강도를 바꾸는 정도 \n",
    "#한글 버전의 경우 불필요한 변수로 생각됨\n",
    "#\n",
    "C_INCR = 0.733\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "#부정적 의미의 동사 형용사 부사 등의 보조어를 모아둔 리스트 (len = 59)\n",
    "#한글의 경우 어떤 용어로 대채할 지 조사 필요함 \n",
    "\n",
    "NEGATE = \\\n",
    "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "\n",
    "#BOOSTER WORD가 감정강도에 긍정, 부정인지 정리한 DICT (len = 84)\n",
    "BOOSTER_DICT = \\\n",
    "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \n",
    "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
    "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
    "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR, \n",
    "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
    "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
    "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \n",
    "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
    "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \n",
    "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR, \n",
    "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
    "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
    "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
    "     \"very\": B_INCR,\n",
    "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
    "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
    "\n",
    "#lexcicon에 포함되지 않은 감성적인 관용적 표현들 모음\n",
    "#본 코드에서는 아직 구현하지 않음.\n",
    "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
    "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                          \"upper hand\": 1, \"break a leg\": 2,\n",
    "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                          \"on the ball\": 2, \"under the weather\": -2}\n",
    "\n",
    "#Lexicon에 포함된 특이 케이스의 관용적 표현들이라는데 무슨 기준인지 잘 모르겠슴....\n",
    "SPECIAL_CASES = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5, \"bus stop\": 0.0,\n",
    "                 \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3, \n",
    "                 \"beating heart\": 3.1, \"broken heart\": -2.9 }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63dda6e3-829c-41f4-b63e-aa6610252bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Static methods 구현.\n",
    "class 객체를 만들지 않아도, 모듈을 import하기만 해도\n",
    "사용할 수 있는 함수들을 뜻함\n",
    "즉, class 선언 외부에 존재하는 함수들임\n",
    "\n",
    "(class 내부에 선언된 함수의 경우 객체를 만들어야 사용가능하고 non-static methods라고 함)\n",
    "'''\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    input_words에 NEGATE(부정형 보조어 리스트)에 속하는 word가 존재 : return True\n",
    "    input_words에 NEGATE(부정형 보조어 리스트)에 속하는 word가 존재 x : return False\n",
    "    \n",
    "    include_nt = True 일때  \"n't\"를 포함하는 단어가 포함되면 retrun True\n",
    "    \"\"\"\n",
    "    input_words = [str(w).lower() for w in input_words]\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    '''원래 주석처리 되어 있던 부분. \"at least\"를 부정형 보조어로 처리할지 결정하지 못한 듯 보임.\n",
    "        if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i - 1] != \"at\":\n",
    "            return True'''\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    score를 정규화 시켜주는 함수\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    if norm_score < -1.0:\n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "    \n",
    "\n",
    "\n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    words에 포함된 일부의 word가 전부 대문자로 표기된 경우가 있으면 return True\n",
    "    한글 버전에서는 구현이 필요 없을 것 같다.\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    valence : word 뒤에 오는 단어의 감성 수치\n",
    "    scalar : word가 BOOSTER_DICT에 포함된 단어일 경우 B_INCR, 또는 B_DECR의 값을 가짐\n",
    "    \n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # word가 대문자인지 여부에 따라 강도 변경하는 부분.\n",
    "        # 한글 버전에서는 불필요함\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cfec2-e6ae-467c-bc33-0d7613e65abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    객체 입력값 : emoji를 text로 변환시킨 text 문자열\n",
    "    \n",
    "    \n",
    "    words_and_emoticons : 구둣점을 제외한 text\n",
    "    is_cap_diff : 대문자 강조 표현이 사용되었는지 여부\n",
    "    text : 처음 입력된 문자열\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text).encode('utf-8')                              #text가 srt이 아니면 str로 변환해준다.\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()            # 구둣점을 제거하고 words와 emoticons을 남긴 text를 word_and_emoticons에 저장\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)  #대문자 강조 표현이 있으면 True 아니면 False, 한글 구현에서는 불필요\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_punc_if_word(token):\n",
    "        \"\"\"\n",
    "        구두점 제거할 때 이모티콘인지 판별하는 함수\n",
    "        아래의 _words_and_emoticons 함수에서 사용함\n",
    "        \"\"\"\n",
    "        stripped = token.strip(string.punctuation)\n",
    "        if len(stripped) <= 2:\n",
    "            return token\n",
    "        return stripped\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        이모티콘을 제외한 선행 후행 구둣점 제거\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        stripped = list(map(self._strip_punc_if_word, wes))\n",
    "        return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e26a7-e5e7-4570-b939-895b04bd0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, lexicon_file=\"vader_lexicon.txt\", emoji_lexicon=\"emoji_utf8_lexicon.txt\"):\n",
    "        '''\n",
    "        lexicon_file : 단어 감성사전 => 한글버전 필요함.\n",
    "        emoji_lexicon : 이모티콘 감성사전 => 그대로 사용할지 말지 판단 필요함.\n",
    "        '''\n",
    "        _this_module_file_path_ = os.path.abspath(getsourcefile(lambda: 0)) #현재 모듈이 실행중인 파일의 절대경로 저장\n",
    "        lexicon_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), lexicon_file) #lexicon_file 경로 저장\n",
    "        with codecs.open(lexicon_full_filepath, encoding='utf-8') as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "        emoji_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), emoji_lexicon) #emoji_lexicon 경로 저장\n",
    "        with codecs.open(emoji_full_filepath, encoding='utf-8') as f:\n",
    "            self.emoji_full_filepath = f.read()\n",
    "        self.emojis = self.make_emoji_dict()\n",
    "        \n",
    "        ''' 결과\n",
    "        self.lexicon  : lexcion dictionary\n",
    "        self.emojis : emojis dictionary\n",
    "        '''\n",
    "    \n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        lexicion파일을 dictionary 형태로 바꿔준다. (__init__에서 사용됨)\n",
    "        \n",
    "        return:\n",
    "        lex_dict = {word : (float(감성점수))}\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "        \n",
    "    def make_emoji_dict(self):\n",
    "        \"\"\"\n",
    "        emojis lexicon파일을 dictionary 형태로 바꿔준다.(__init__에서 사용됨)\n",
    "        \n",
    "        return:\n",
    "        emoji_dict = {emoji : emoji의 의미(text)}\n",
    "        \"\"\"\n",
    "        emoji_dict = {}\n",
    "        for line in self.emoji_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            (emoji, description) = line.strip().split('\\t')[0:2]\n",
    "            emoji_dict[emoji] = description\n",
    "        return emoji_dict\n",
    "    \n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        text기준으로 감정 강도를 float 점수로 반환해줌.\n",
    "        \n",
    "        \"\"\"\n",
    "        # emoji를 텍스트로 변환하는 작업\n",
    "        text_no_emoji = \"\"\n",
    "        prev_space = True\n",
    "        for chr in text:\n",
    "            if chr in self.emojis:              #text의 문자가 emoji일 경우\n",
    "                description = self.emojis[chr]  #description에 emoji 뜻 저장\n",
    "                if not prev_space:              #emoji 앞에 여백(space)가 없으면\n",
    "                    text_no_emoji += ' '        #text_no_emoji에 여백을 만들어주고\n",
    "                text_no_emoji += description    #text_no_emoji에 decription 붙여준다.\n",
    "                prev_space = False\n",
    "            else:                               #emoji가 아닐 경우\n",
    "                text_no_emoji += chr            #text_no_emoji에 chr 추가\n",
    "                prev_space = chr == ' '         #문자가 공백으로 입력된 경우 prev_space == True로 바꾸어줌\n",
    "        text = text_no_emoji.strip()            #최종 결과 text_no_emoji의 공백 제거 후 text에 다시 저장\n",
    "\n",
    "        sentitext = SentiText(text)             # 구둣점 제거한 text, 대문자 강조 사용 여부를 저장하고 있는 객체 생성\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for i, item in enumerate(words_and_emoticons):\n",
    "            \n",
    "            valence = 0                         #valence 감성 수치(점수)\n",
    "            \n",
    "            if item.lower() in BOOSTER_DICT:    #item이 BOOSTER_DICT에 있으면 0\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and  #현재 item이 \"kind\"이고 다음 item이 \"of\"이면 0\n",
    "                    words_and_emoticons[i + 1].lower() == \"of\"):                 #아마 \"kind\"가 친철한 이라는 의미도 가지고 있어서 넣어놓은 알고리즘인 듯\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments) #위의 조건에 해당하지 않으면 snetiment_valence 함수를 사용하여 valence 값 넣기\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments) #문장에 \"but\"이 있는지 확인하고 전체적인 valence 조정\n",
    "\n",
    "        valence_dict = self.score_valence(sentiments, text) #각각의 단어로부터 구한 valence를 저장한 리스트 sentiments로부터 최종적인 문장의 감성 강도 계산\n",
    "\n",
    "        return valence_dict #valence_dict는 neg, neu, pos, compound 값을 가지고 있음\n",
    "    \n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        '''\n",
    "        valence : 감성 수치 변수\n",
    "        sentitext : 구둣점 제거한 text, 대문자 강조 사용 여부를 저장하고 있는 객체\n",
    "        item : valence를 계산할 단어\n",
    "        i : text에서 item의 index\n",
    "        sentiments : valence를 추가하여 return할 list\n",
    "        \n",
    "        return\n",
    "        : sentiments\n",
    "        '''\n",
    "        is_cap_diff = sentitext.is_cap_diff    #대문자 강조 표현이 text에 있는지 확인                                                                   \n",
    "        words_and_emoticons = sentitext.words_and_emoticons \n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "                \n",
    "            # 수식하는 용도의 \"no\" 와 독립 어휘로서의 \"no\"를 체크하는 부분\n",
    "            if item_lowercase == \"no\" and i != len(words_and_emoticons)-1 and words_and_emoticons[i + 1].lower() in self.lexicon: # item이 \"no\"이고 마지막 단어가 아닐떄, 그리고 다음 item이 lexicon에 있을경우\n",
    "                # \"no\"를 lexicon의 valence로 사용하지 않음(수식어로 체크)\n",
    "                valence = 0.0\n",
    "            #현재 item 앞에 \"no\"가 있으면 N_SCALAR를 valence에 곱한다.\n",
    "            if (i > 0 and words_and_emoticons[i - 1].lower() == \"no\") \\\n",
    "               or (i > 1 and words_and_emoticons[i - 2].lower() == \"no\") \\\n",
    "               or (i > 2 and words_and_emoticons[i - 3].lower() == \"no\" and words_and_emoticons[i - 1].lower() in [\"or\", \"nor\"] ):\n",
    "                valence = self.lexicon[item_lowercase] * N_SCALAR\n",
    "            \n",
    "            # 대문자 강조 표현에 valence 조정 (한글 구현에 필요 x)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "            \n",
    "            for start_i in range(0, 3):\n",
    "                # 현재 item 앞에 온 booster_dict이 존재하는지, 거리가 얼마나 되는지에 따라 valence 조정한다. \n",
    "                if i > start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon:\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff) #s : valence를 조정할 scalar 값\n",
    "                    if start_i == 1 and s != 0:  #item과 booster_word와의 거리에 따른 스칼라값 조정\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s       #최종적으로 구해진 scalar값으로 valence 조정\n",
    "                    valence = self._negation_check(valence, words_and_emoticons, start_i, i) #부정문인지 체크하여 valence 조정\n",
    "                    if start_i == 2:\n",
    "                        valence = self._special_idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "            valence = self.least_check(valence, words_and_emoticons, i) #item 앞에 단어가 \"least\"인지에 따라 valence 조정\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            if words_and_emoticons[i - 2].lower() != \"at\" and words_and_emoticons[i - 2].lower() != \"very\":\n",
    "                valence = valence * N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _but_check(words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if 'but' in words_and_emoticons_lower:\n",
    "            bi = words_and_emoticons_lower.index('but')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 1.5)\n",
    "        return sentiments\n",
    "\n",
    "    @staticmethod\n",
    "    def _special_idioms_check(valence, words_and_emoticons, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 2],\n",
    "                                          words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 3],\n",
    "                                           words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons_lower[i - 3], words_and_emoticons_lower[i - 2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons_lower) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1])\n",
    "            if zeroone in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroone]\n",
    "        if len(words_and_emoticons_lower) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1],\n",
    "                                              words_and_emoticons_lower[i + 2])\n",
    "            if zeroonetwo in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        n_grams = [threetwoone, threetwo, twoone]\n",
    "        for n_gram in n_grams:\n",
    "            if n_gram in BOOSTER_DICT:\n",
    "                valence = valence + BOOSTER_DICT[n_gram]\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentiment_laden_idioms_check(valence, senti_text_lower):\n",
    "        # Future Work\n",
    "        # check for sentiment laden idioms that don't contain a lexicon word\n",
    "        idioms_valences = []\n",
    "        for idiom in SENTIMENT_LADEN_IDIOMS:\n",
    "            if idiom in senti_text_lower:\n",
    "                print(idiom, senti_text_lower)\n",
    "                valence = SENTIMENT_LADEN_IDIOMS[idiom]\n",
    "                idioms_valences.append(valence)\n",
    "        if len(idioms_valences) > 0:\n",
    "            valence = sum(idioms_valences) / float(len(idioms_valences))\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _negation_check(valence, words_and_emoticons, start_i, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 1 word preceding lexicon word (w/o stopwords)\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons_lower[i - 2] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or\n",
    "                     words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 2] == \"without\" and \\\n",
    "                    words_and_emoticons_lower[i - 1] == \"doubt\":\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 2 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons_lower[i - 3] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"so\" or words_and_emoticons_lower[i - 2] == \"this\") or \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 3] == \"without\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"doubt\" or words_and_emoticons_lower[i - 1] == \"doubt\"):\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 3 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, text):\n",
    "        # !,? 갯수 (최댓값 4)에 따라 punct_emph_amplifier 값 반환\n",
    "        ep_amplifier = self._amplify_ep(text) # ! 개수 따른 값\n",
    "        qm_amplifier = self._amplify_qm(text) # ? 개수 따른 값\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_ep(text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_qm(text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_sentiment_scores(sentiments):\n",
    "        \"\"\"\n",
    "        pos_sum : sentiments에서 0보다 큰 요소들의 합\n",
    "        neg_sum : sentiments에서 0보다 작은 요소들의 합\n",
    "        neu_count : 요소의 값이 0인 것 의 갯수\n",
    "        \"\"\"\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) + 1)  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) - 1)  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # !,?에 따른 valence 조정값\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier #긍정 문장이면 더하고\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier #부정 문장이면 뺀다.\n",
    "\n",
    "            compound = normalize(sum_s) # sum_s 를 최대 1, 최소 -1로 정규화\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "            \n",
    "            #pos_sum과 neg_sum의 절대값을 비교\n",
    "            if pos_sum > math.fabs(neg_sum):   #pos_sum이 크면 pos_sum에 ?!강조에 따른 valence 조정 더함\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum): #neg_sum이 크면 neg_sum에 ?!강조에 따른 valence 조정 뺌\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\": round(neg, 3),\n",
    "             \"neu\": round(neu, 3),\n",
    "             \"pos\": round(pos, 3),\n",
    "             \"compound\": round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
